{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3163,"status":"ok","timestamp":1646076998365,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"8eYMSpm0k-dr"},"outputs":[],"source":["import os\n","import zipfile\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import math\n","import tensorflow as tf\n","\n","from PIL import Image\n","from matplotlib import pyplot\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, SeparableConv2D, BatchNormalization, MaxPooling2D, Dense, Dropout, Flatten, Activation\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.models import load_model"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1048,"status":"ok","timestamp":1646079701776,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"5qpkVaFBk-dv"},"outputs":[],"source":["def unzip_samples(file = \"data.zip\", overwrite = False):\n","    if not os.path.exists(os.path.splitext(file)[0]) or overwrite:\n","        with zipfile.ZipFile(file, 'r') as zip_ref:\n","            zip_ref.extractall(\".\")\n","\n","def generate_datasets(zip_data_path = \"data.zip\", train_set_path = \"train_set\", train_set_size = 200, test_set_path = \"test_set\", test_set_size = 50, validation_set_path = \"validation_set\", validation_set_size = 50, channels = 4, crop_area = (0, 0, 32, 32), size = (32, 32), unzip = True):\n","    if unzip:\n","        unzip_samples(zip_data_path)\n","    generate_tensors(train_set_path, sample_start = 1, sample_end = train_set_size, channels = channels, crop_area = crop_area, size = size)\n","    generate_tensors(test_set_path, sample_start = 1, sample_end = test_set_size, channels = channels, crop_area = crop_area, size = size)\n","    generate_tensors(validation_set_path, sample_start = test_set_size + 1, sample_end = test_set_size + validation_set_size, channels = channels, crop_area = crop_area, size = size)\n","\n","def sample_path(class_name, sample, channel, size):\n","    png_path = \"dataset\" + str(size[1]) + \"x\" + str(size[0]) + \"/\" + class_name + \"/CH\" + str(channel) + \"/sample\" + str(sample) + \".png\"\n","    return png_path\n","\n","def show_sample(class_name, sample, crop_area = (0, 0, 32, 32), size = (32, 32)):\n","    print(\"Sample = {}, class = {}\".format(sample, class_name))\n","    fig, ax = plt.subplots(1, 7)\n","    fig.set_figwidth(20)\n","    fig.set_figheight(10)\n","    for i in range(7):\n","        im = Image.open(sample_path(class_name = class_name, sample = sample, channel = i+1), \"r\").crop(crop_area).resize(size)\n","        ax[i].set_title(\"Ch {}\".format(i+1))\n","        ax[i].imshow(im)\n","    fig.show()\n","\n","def show_grid(sample, channels = 4, crop_area = (0, 0, 32, 32), size = (32, 32)):\n","    print(\"Grid (sample = {}, channels = {})\".format(sample, channels))\n","    fig, ax = plt.subplots(10, 10)\n","    fig.set_figwidth(20)\n","    fig.set_figheight(24)\n","    matrix = np.zeros((size[1], size[0], channels), dtype=\"uint8\")\n","    for row in range(10):\n","        for col in range(10):\n","            if (row == 0 and col == 0) or (row == 0 and col == 9) or (row == 9 and col == 0) or (row == 9 and col == 9):\n","                    continue\n","            class_name = \"P_x\" + str(row + 1) + \"_y\" + str(col + 1)\n","            for ch in range(channels):\n","                matrix[:, :, ch] = np.array(Image.open(sample_path(class_name = class_name, sample = sample, channel = ch + 1), \"r\").crop(crop_area).resize(size))\n","            ax[row, col].set_title(class_name)\n","            ax[row, col].imshow(matrix)\n","    plt.show()\n","\n","def generate_sample_tensor(class_name = \"A11\", sample = 1, channels = 4, crop_area = (0, 0, 32, 32), size = (32, 32)):\n","    tensor = np.zeros((size[1], size[0], channels), dtype=\"uint8\")\n","    for ch in range(channels):\n","        img = Image.open(sample_path(class_name = class_name, sample = sample, channel = ch + 1, size = size), \"r\")\n","        img = img.crop(crop_area)\n","        img = img.resize(size)\n","        tensor[:, :, ch] = np.array(img)\n","    return tensor\n","\n","def generate_tensors(dest_path = \"tensors\", sample_start = 1, sample_end = 300, channels = 4, crop_area = (0, 0, 32, 32), size = (32, 32), rows = 10, cols = 10, overwrite = False):\n","    if not os.path.exists(dest_path) or overwrite:\n","        for row in range(rows):\n","            for col in range(cols):\n","                if (row == 0 and col == 0) or (row == 0 and col == 9) or (row == 9 and col == 0) or (row == 9 and col == 9):\n","                    continue\n","                class_name = \"P_x\" + str(col + 1) + \"_y\" + str(row + 1)\n","                path = os.path.join(dest_path, class_name)\n","                if not os.path.exists(path):\n","                    os.makedirs(path)\n","                for i in range(sample_start, sample_end + 1):\n","                    file_name = os.path.join(path, \"sample_\" + str(i))\n","                    tensor = generate_sample_tensor(class_name = class_name, sample = i, channels = channels, crop_area = crop_area, size = size)\n","                    np.save(file_name, tensor)\n","\n","def create_labeled_data(srs_path = \"tensors\", label_type = \"categorical\", category_group = None):\n","    labeled_data = []\n","    categories = []\n","    \n","    if category_group != None:\n","        categories = category_group\n","    else:\n","        categories = os.listdir(srs_path)\n","        categories.sort()\n","\n","    for category in categories:\n","        path = os.path.join(srs_path, category)  # create path to the classes\n","        class_num = categories.index(category)  # get the classification  0 = P_x10_y2, 1 = A12, 2 = A13, ...etc\n","\n","        file_list = os.listdir(path)\n","        file_list.sort()\n","        for file in file_list:\n","            file = os.path.join(path, file)\n","            sample = np.load(file)\n","            labeled_data.append([sample, class_num])\n","\n","    random.shuffle(labeled_data)\n","\n","    X = []\n","    Y = []\n","    for sample, label in labeled_data:\n","        X.append(sample)\n","        Y.append(label)\n","\n","    X = np.array(X)\n","    X = X.astype(\"float32\")/255\n","\n","    if label_type == \"regression\":\n","        Y = to_regression(Y, categories)\n","    else:\n","        Y = np.array(Y)\n","        np.reshape(Y, (len(Y), 1))\n","        Y = to_categorical(Y)\n","\n","    return X, Y\n","\n","def to_regression(categorical_labels, categories):\n","    tensor = np.zeros((len(categorical_labels), 2))\n","    for i in range(len(categorical_labels)):\n","        category_name = categories[categorical_labels[i]]\n","\n","        x_i = int(category_name.split(\"_\")[1].split(\"x\")[1]) - 1\n","        y_i = int(category_name.split(\"_\")[2].split(\"y\")[1]) - 1\n","\n","        x_pos = 0.0935 * x_i + 0.012\n","        y_pos = 0.0973 * y_i + 0.012\n","        tensor[i,:] = [x_pos, y_pos]\n","    return tensor\n","\n","def get_category(distributions, max_row = 4, max_col = 4):\n","    category_index = distributions.argmax()\n","\n","    col = category_index % max_col\n","    row = int((category_index - col) / max_row)\n","    category = \"A\" + str(row + 1) + str(col + 1)\n","    return category\n","\n","def optimization_plot(history, filename):\n","    # plot loss\n","    f = pyplot.figure()\n","    f.set_figwidth(10)\n","    f.set_figheight(20)\n","\n","    pyplot.subplot(211)\n","    pyplot.title('Cross Entropy Loss')\n","    pyplot.plot(history.history['loss'], color='blue', label='train')\n","    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n","    # plot accuracy\n","    pyplot.subplot(212)\n","    pyplot.title('Regression MSE')\n","    pyplot.plot(history.history['mean_squared_error'], color='blue', label='train')\n","    pyplot.plot(history.history['val_mean_squared_error'], color='orange', label='test')\n","    # save plot to file\n","    pyplot.savefig(filename + '.png')\n","    pyplot.close()\n","\n","def compute_error(error_tensor):\n","    mse = ((error_tensor)**2).mean(axis=None)\n","    max_error = np.max(error_tensor)\n","    ave_error = sum(error_tensor) / len(error_tensor)\n","    return mse, ave_error, max_error\n","\n","def evaluate_regression(model, val_X, val_Y, accuracy_threshold):\n","    model_output = model.predict(val_X)\n","    error_tensor = val_Y - model_output\n","    error_x_tensor = error_tensor[:,0]\n","    error_y_tensor = error_tensor[:,1]\n","    loss_distance = np.sqrt(error_x_tensor**2 + error_y_tensor**2)\n","    mse, ave_error, max_error = compute_error (loss_distance)\n","    accuracy = np.count_nonzero(loss_distance < accuracy_threshold) / len(loss_distance)\n","    return mse, ave_error, max_error, accuracy\n","\n","def evaluate_regression_TF_Lite(interpreter, val_X, val_Y, accuracy_threshold):\n","    # Get input and output tensors.\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    val_Y_lite = np.zeros((len(val_Y), 2))\n","    for i in range(len(val_X)):\n","        interpreter.set_tensor(input_details[0]['index'], np.expand_dims(val_X[i], axis=0))\n","        interpreter.invoke()\n","        output_data = interpreter.get_tensor(output_details[0]['index'])\n","        val_Y_lite[i] = output_data\n","\n","    error_tensor_lite = val_Y - val_Y_lite\n","\n","    error_x_tensor_lite = error_tensor_lite[:,0]\n","    error_y_tensor_lite = error_tensor_lite[:,1]\n","\n","    loss_distance_lite = np.sqrt(error_x_tensor_lite**2 + error_y_tensor_lite**2)\n","\n","    accuracy_lite = np.count_nonzero(loss_distance_lite < accuracy_threshold) / len(loss_distance_lite)\n","    mse_lite, ave_error_lite, max_error_lite = compute_error(loss_distance_lite)\n","    return mse_lite, ave_error_lite, max_error_lite, accuracy_lite\n","\n","def train_model(model, model_file_name = \"model.h5\", target_mse = 0.000001, accuracy_threshold = 0.10):\n","    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=True)\n","\n","    current_mse = 1\n","    while (target_mse < current_mse):\n","        history = model.fit(train_X, train_Y, epochs=100, batch_size=1000, validation_data=(val_X, val_Y), verbose=1, callbacks=[monitor])\n","        validation_mse_, ave_error_, max_error_, validation_accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","        if (validation_mse_ < current_mse):\n","            print(\"Validation MSE > {}\".format(validation_mse_))\n","            print(\"Saving model > {}\".format(model_file_name))\n","            model.save(model_file_name)\n","            optimization_plot(history, \"optimization\")\n","            current_mse = validation_mse_\n","        else:\n","            if os.path.exists(model_file_name):\n","                model = load_model(model_file_name)\n","\n","    print(\"{} has been trained, MSE = {}.\".format(model_file_name, current_mse))\n","\n","def convert_to_TF_Lite_float32 (model_file_name):\n","  model = load_model(model_file_name)\n","\n","  # Convert the model.\n","  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","  converter.inference_input_type = tf.float32  # or tf.uint8\n","  converter.inference_output_type = tf.float32  # or tf.uint8\n","  tflite_model = converter.convert()\n","\n","  # Save the model.\n","  with open(model_file_name + '.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","\n","  os.system(\"xxd -i \" + model_file_name + '.tflite' + \" > \" + model_file_name + '.cpp')\n","\n","  print(\"{} has been converted to TF Lite (float32).\".format(model_file_name))\n","\n","def convert_to_TF_Lite_int8 (model_file_name, val_X):\n","  ds_cast = tf.cast(val_X, tf.float32)\n","  ds_batch = tf.data.Dataset.from_tensor_slices((ds_cast)).batch(1)\n","  def representative_dataset():\n","    for input_value in ds_batch.take(100):\n","      # Model has only one input so each data point has one element.\n","      yield [input_value]\n","\n","  model = load_model(model_file_name)\n","  # Convert the model.\n","  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","  converter.representative_dataset = representative_dataset\n","  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","  converter.inference_input_type = tf.float32  # or tf.uint8\n","  converter.inference_output_type = tf.float32  # or tf.uint8\n","  tflite_model = converter.convert()\n","\n","  # Save the model.\n","  with open(model_file_name + \"_i8\" + '.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","\n","  print(\"{} has been converted to TF Lite (int8).\".format(model_file_name))\n","\n","def create_custom_training_set (srs_path, tokens):\n","    categories = os.listdir(srs_path)\n","    training_categories = []\n","    validation_categories = []\n","    for category in categories:\n","        is_train_category = False\n","        for token in tokens:\n","            if token in category:\n","                training_categories.append(category)\n","                is_train_category = True\n","        if not is_train_category:\n","            validation_categories.append(category)\n","\n","    return list (dict.fromkeys(training_categories)), list (dict.fromkeys(validation_categories))\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":358,"status":"ok","timestamp":1646080824052,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"Mxi14fO0k-d0"},"outputs":[],"source":["import struct\n","\n","def bin2float(b):\n","    ''' Convert binary string to a float.\n","\n","    Attributes:\n","        :b: Binary string to transform.\n","    '''\n","    h = int(b, 2).to_bytes(8, byteorder=\"big\")\n","    return struct.unpack('>d', h)[0]\n","\n","\n","def float2bin(f):\n","    ''' Convert float to 64-bit binary string.\n","\n","    Attributes:\n","        :f: Float number to transform.\n","    '''\n","    [d] = struct.unpack(\">Q\", struct.pack(\">d\", f))\n","    return f'{d:064b}'\n","\n","def quantize_float (float_number, exponent_bits, mantissa_bits):\n","    exponent_sign = 1\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n","    if mantissa_bits < 0:\n","      mantissa_bits = 0\n","    if exponent_bits < 0:\n","      exponent_bits = 0\n","      \n","    coefficient = float2bin(float_number)\n","    sign = int (coefficient[:1], 2)\n","    if 0 < mantissa_bits:\n","        custom_mantissa = int (coefficient[12 : 12 + mantissa_bits], 2)\n","    else:\n","        custom_mantissa = 0\n","    residual_mantissa = int (coefficient[12 + mantissa_bits:], 2)\n","    exponent = int (coefficient[1:12], 2) - 1023\n","\n","    exponent_full_range = pow(2, exponent_bits - exponent_sign) - 1\n","    if exponent < - exponent_full_range:\n","        quantized_value = 0\n","    elif exponent > exponent_full_range:\n","        quantized_value = pow(-1, sign) * (1 + (1 - pow(2, - mantissa_bits))) * pow(2, exponent_full_range)\n","    else:\n","        if (pow (2, (52 - (mantissa_bits + 1))) - 1) < residual_mantissa:\n","            custom_mantissa += 1\n","            if (pow (2, mantissa_bits) - 1) < custom_mantissa:\n","                custom_mantissa = 0\n","                exponent += 1\n","    \n","        quantized_value = pow(-1, sign) * (1 + custom_mantissa * pow(2, - mantissa_bits)) * pow(2, exponent)\n","    return quantized_value\n","\n","def quantize_model(model, exponent_bits, mantissa_bits):\n","  for layer in model.layers:\n","    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.SeparableConv2D):\n","      layer_weights = layer.get_weights()\n","      for index in range(len(layer_weights)):\n","        matrix = layer_weights[index]\n","        for weight_index, weight in np.ndenumerate(matrix):\n","          matrix[weight_index] = quantize_float(weight, exponent_bits, mantissa_bits)\n","        layer_weights[index] = matrix\n","      layer.set_weights(layer_weights)\n","\n","class QuantizeCallback(tf.keras.callbacks.Callback):\n","  def __init__(self, model_file_name, exponent_bits, mantissa_bits, current_mse, threshold_accuracy, refine_on_batch, quantize_enable):\n","    self.model_file_name = model_file_name\n","    self.threshold_accuracy = threshold_accuracy\n","    self.current_mse = current_mse\n","    self.refine_on_batch = refine_on_batch\n","    self.exponent_bits = exponent_bits\n","    self.mantissa_bits = mantissa_bits\n","    self.quantize_enable = quantize_enable\n","\n","  def on_epoch_end(self, epoch, logs = None):\n","    if self.quantize_enable:\n","      quantize_model(self.model, self.exponent_bits, self.mantissa_bits)\n","    test_mse_, ave_error_, max_error_, test_accuracy = evaluate_regression(model = self.model, val_X = val_X, val_Y = val_Y, accuracy_threshold = self.threshold_accuracy)\n","    if (test_mse_ < self.current_mse):\n","      self.model.save(self.model_file_name)\n","      self.current_mse = test_mse_\n","      print(\" Saveing model ...\")\n","\n","    print(\" Regression MSE = {}\".format(test_mse_))\n","  def on_train_end(self, logs = None):\n","    if self.quantize_enable:\n","      quantize_model(self.model, self.exponent_bits, self.mantissa_bits)\n","  def on_batch_end(self, epoch, logs = None):\n","    if self.quantize_enable:\n","      quantize_model(self.model, self.exponent_bits, self.mantissa_bits)\n","\n","    if self.refine_on_batch:\n","      test_mse_, ave_error_, max_error_, test_accuracy = evaluate_regression(model = self.model, val_X = val_X, val_Y = val_Y, accuracy_threshold = self.threshold_accuracy)\n","      if (test_mse_ < self.current_mse):\n","        self.model.save(self.model_file_name)\n","        self.current_mse = test_mse_\n","        print(\" Saveing model ...\")\n","        print(\" Regression MSE {}\".format(test_mse_))\n","\n","def print_model(model):\n","  for layer in model.layers:\n","    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.SeparableConv2D):\n","      layer_weights = layer.get_weights()\n","      print (layer_weights)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1646076999098,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"1U3sJkhSk-d2"},"outputs":[],"source":["def CNN_regression(input_shape = (32, 32, 4), output_regression = 2):\n","    model = Sequential()\n","\n","    model.add(Conv2D(40, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape = input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Dropout(0.3))\n","\n","    model.add(Conv2D(60, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Dropout(0.3))\n","\n","    model.add(Conv2D(120, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Dropout(0.3))\n","\n","    model.add(Flatten())\n","    model.add(Dense(32, activation='relu'))\n","    model.add(Dense(output_regression, activation='sigmoid'))\n","    # compile model\n","    #opt = SGD(learning_rate=0.001, momentum=0.9)\n","    opt = \"adam\"\n","    model.compile(optimizer=opt, loss='mean_squared_error', metrics=['MeanSquaredError'])\n","    return model"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1823,"status":"ok","timestamp":1646077000913,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"B2YTXhvbk-d3","outputId":"732139c7-bd58-4dfc-956f-53346dc76f6e"},"outputs":[],"source":["google_service = False\n","\n","absolute_path = \"\"\n","\n","if google_service:\n","    from google.colab import drive\n","    google_drive_path = \"/content/drive/MyDrive/PhD/Minized/\"\n","    drive.mount('/content/drive/')\n","    absolute_path = google_drive_path\n","\n","channel_sensors = 6\n","\n","temporal_resolution = 16\n","\n","model_file_name = absolute_path + \"model_8x\" + str(temporal_resolution) + \"x\" + str(channel_sensors) + \".h5\" # Set the file name for the model\n","target_mse = 0.045\n","accuracy_threshold = 0.15 # Radio-distance (in meters) to consider a correct regression\n","\n","model_type = \"regression\" # set \"regression\" or \"categorical\"\n","\n","zip_data_path = absolute_path + \"dataset8x\" + str(temporal_resolution) + \".zip\"\n","unzip_dataset = True\n","\n","train_set_path = absolute_path + \"train_set\"\n","train_set_size = 2000\n","\n","test_set_path = absolute_path + \"test_set\"\n","test_set_size = 2000\n","\n","validation_set_path = absolute_path + \"validation_set\"\n","validation_set_size = 500\n","\n","crop_area = (0, 0, temporal_resolution, 8) # x1, y1, x2, y2\n","size = (temporal_resolution, 8)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":261681,"status":"ok","timestamp":1646077262588,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"bf6wEFzIk-d3"},"outputs":[],"source":["generate_datasets ( zip_data_path = zip_data_path,\\\n","                    train_set_path = train_set_path,\\\n","                    train_set_size = train_set_size,\\\n","                    test_set_path = test_set_path,\\\n","                    test_set_size = test_set_size,\\\n","                    validation_set_path = validation_set_path,\\\n","                    validation_set_size = validation_set_size,\\\n","                    channels = channel_sensors,\\\n","                    crop_area = crop_area,\\\n","                    size = size,\n","                    unzip = unzip_dataset)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1646077262590,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"xnShZa-Vk-d4"},"outputs":[],"source":["#tokens = [\"x1_\", \"x10_\", \"_y1\", \"_y10\", \"x5_\", \"_y5\"]\n","tokens = [\"x3_y3\", \"x5_y2\", \"x8_y3\", \"x6_y4\", \"x4_y5\", \"x9_y5\", \"x2_y6\", \"x5_y7\", \"x8_y8\", \"x3_y9\", \"x6_y9\", \"x7_y6\", \"x1_y2\", \"x10_y9\"]\n","\n","training_group, test_group = create_custom_training_set (srs_path = train_set_path, tokens = tokens)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":138259,"status":"ok","timestamp":1646077400836,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"PWMMyapzk-d4"},"outputs":[],"source":["train_X, train_Y = create_labeled_data(srs_path = train_set_path, label_type = model_type, category_group = training_group)\n","test_X, test_Y = create_labeled_data(srs_path = test_set_path, label_type = model_type, category_group = test_group)\n","val_X, val_Y = create_labeled_data(srs_path = validation_set_path, label_type = model_type)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1646077400837,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"lqoYYYWpk-d5","outputId":"b0f66234-2de9-499d-ae3e-f5b1572d180f"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.scatter(train_Y[:,0], train_Y[:,1], c='r')\n","plt.show()"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2330,"status":"ok","timestamp":1646077403162,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"6VUMfV_Qk-d6","outputId":"1fe22b8d-5f68-4740-fdd7-69e4e6413be4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_3 (Conv2D)            (None, 8, 16, 40)         2200      \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 8, 16, 40)         160       \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 4, 8, 40)          0         \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 4, 8, 40)          0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 4, 8, 60)          21660     \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 4, 8, 60)          240       \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 2, 4, 60)          0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 2, 4, 60)          0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 2, 4, 120)         64920     \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 2, 4, 120)         480       \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 1, 2, 120)         0         \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 1, 2, 120)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 240)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 32)                7712      \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 2)                 66        \n","=================================================================\n","Total params: 97,438\n","Trainable params: 96,998\n","Non-trainable params: 440\n","_________________________________________________________________\n","('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"]}],"source":["model = CNN_regression(input_shape = (size[1], size[0], channel_sensors))\n","\n","model.summary()\n","\n","tf.keras.utils.plot_model(model, model_file_name + \".png\", show_shapes=True, show_layer_names=True, expand_nested=True)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":169953,"status":"error","timestamp":1646079880611,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"tiq18IU3k-d6","outputId":"091227b7-4eb0-4f2e-af7a-6df6ed0ebea7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","25/25 [==============================] - 24s 972ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n","Epoch 2/100\n","25/25 [==============================] - 24s 967ms/step - loss: 0.0127 - mean_squared_error: 0.0127 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n","Epoch 3/100\n","25/25 [==============================] - 22s 899ms/step - loss: 0.0103 - mean_squared_error: 0.0103 - val_loss: 0.1364 - val_mean_squared_error: 0.1364\n","Epoch 4/100\n","25/25 [==============================] - 24s 952ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.2128 - val_mean_squared_error: 0.2128\n","Epoch 5/100\n","25/25 [==============================] - 24s 974ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.2763 - val_mean_squared_error: 0.2763\n","Epoch 6/100\n","25/25 [==============================] - 24s 966ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.2856 - val_mean_squared_error: 0.2856\n","Epoch 7/100\n","25/25 [==============================] - 23s 926ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.2785 - val_mean_squared_error: 0.2785\n","Epoch 8/100\n","25/25 [==============================] - 22s 893ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.2561 - val_mean_squared_error: 0.2561\n","Epoch 9/100\n","25/25 [==============================] - 23s 924ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.2017 - val_mean_squared_error: 0.2017\n","Epoch 10/100\n","25/25 [==============================] - 23s 937ms/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.1674 - val_mean_squared_error: 0.1674\n","Epoch 11/100\n","25/25 [==============================] - 22s 880ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.1109 - val_mean_squared_error: 0.1109\n","Epoch 12/100\n","25/25 [==============================] - 23s 926ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0670 - val_mean_squared_error: 0.0670\n","Restoring model weights from the end of the best epoch.\n","Epoch 00012: early stopping\n","Validation MSE > 0.10457930686505079\n","Saving model > model_8x16x6.h5\n","Epoch 1/100\n","25/25 [==============================] - 22s 895ms/step - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.1140 - val_mean_squared_error: 0.1140\n","Epoch 2/100\n","25/25 [==============================] - 23s 910ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.2029 - val_mean_squared_error: 0.2029\n","Epoch 3/100\n","25/25 [==============================] - 23s 933ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.2668 - val_mean_squared_error: 0.2668\n","Epoch 4/100\n","25/25 [==============================] - 19s 777ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.2728 - val_mean_squared_error: 0.2728\n","Epoch 5/100\n","25/25 [==============================] - 24s 945ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.2644 - val_mean_squared_error: 0.2644\n","Epoch 6/100\n","25/25 [==============================] - 21s 846ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.2469 - val_mean_squared_error: 0.2469\n","Epoch 7/100\n","25/25 [==============================] - 21s 863ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.2044 - val_mean_squared_error: 0.2044\n","Epoch 8/100\n","25/25 [==============================] - 22s 887ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.1422 - val_mean_squared_error: 0.1422\n","Epoch 9/100\n","25/25 [==============================] - 23s 941ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0916 - val_mean_squared_error: 0.0916\n","Epoch 10/100\n","25/25 [==============================] - 23s 945ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n","Epoch 11/100\n","25/25 [==============================] - 22s 886ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n","Epoch 12/100\n","25/25 [==============================] - 22s 899ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0212 - val_mean_squared_error: 0.0212\n","Epoch 13/100\n","25/25 [==============================] - 23s 944ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0145 - val_mean_squared_error: 0.0145\n","Epoch 14/100\n","25/25 [==============================] - 23s 907ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0121 - val_mean_squared_error: 0.0121\n","Epoch 15/100\n","25/25 [==============================] - 22s 905ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0112 - val_mean_squared_error: 0.0112\n","Epoch 16/100\n","25/25 [==============================] - 22s 884ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0103 - val_mean_squared_error: 0.0103\n","Epoch 17/100\n","25/25 [==============================] - 23s 914ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n","Epoch 18/100\n","25/25 [==============================] - 22s 864ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n","Epoch 19/100\n","25/25 [==============================] - 21s 855ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0089 - val_mean_squared_error: 0.0089\n","Epoch 20/100\n","25/25 [==============================] - 23s 905ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0089 - val_mean_squared_error: 0.0089\n","Epoch 21/100\n","25/25 [==============================] - 22s 881ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n","Epoch 22/100\n","25/25 [==============================] - 22s 895ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n","Epoch 23/100\n","25/25 [==============================] - 21s 855ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n","Epoch 24/100\n","25/25 [==============================] - 22s 869ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n","Epoch 25/100\n","25/25 [==============================] - 22s 903ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n","Epoch 26/100\n","25/25 [==============================] - 21s 861ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n","Epoch 27/100\n","25/25 [==============================] - 21s 835ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n","Epoch 28/100\n","25/25 [==============================] - 22s 896ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n","Epoch 29/100\n","25/25 [==============================] - 22s 872ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n","Restoring model weights from the end of the best epoch.\n","Epoch 00029: early stopping\n","Validation MSE > 0.01783609721261532\n","Saving model > model_8x16x6.h5\n","model_8x16x6.h5 has been trained, MSE = 0.01783609721261532.\n"]}],"source":["train_model(model = model, model_file_name = model_file_name, target_mse = target_mse, accuracy_threshold = accuracy_threshold)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1646079885196,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"IPzSoupPveEa","outputId":"b369e7b7-1875-4923-d310-6adbc14b5bef"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.scatter(val_Y[:,0], val_Y[:,1], c='b')\n","plt.scatter(train_Y[:,0], train_Y[:,1], c='r')\n","plt.show()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2219,"status":"ok","timestamp":1646079891491,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"6eeHuHxDk-d7","outputId":"f7f0806c-e8a8-4315-a62a-61d25dc05e0d"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-03-07 12:01:26.009726: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2022-03-07 12:01:26.010492: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-03-07 12:01:26.011265: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n","2022-03-07 12:01:26.399573: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n","2022-03-07 12:01:26.422140: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3293910000 Hz\n"]},{"name":"stdout","output_type":"stream","text":["Sensors = 6\n","MSE = 0.013908633454733603\n","Ave error = 0.0988236227687793\n","Max error = 0.48310075258570545\n","Accuracy = 0.7871944444444444\n"]}],"source":["model = load_model(model_file_name)\n","\n","mse, ave_error, max_error, accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","\n","print(\"Sensors = {}\".format(channel_sensors))\n","print(\"MSE = {}\".format(mse))\n","print(\"Ave error = {}\".format(ave_error))\n","print(\"Max error = {}\".format(max_error))\n","print(\"Accuracy = {}\".format(accuracy))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4828,"status":"ok","timestamp":1646079901718,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"8l69_K0Ek-d7","outputId":"dce4092f-a63b-4f60-b937-24f180123969"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-03-07 12:01:32.476991: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: /tmp/tmpgf8vqkou/assets\n","model_8x16x6.h5 has been converted to TF Lite (float32).\n"]},{"name":"stderr","output_type":"stream","text":["2022-03-07 12:01:33.528919: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n","2022-03-07 12:01:33.529220: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n","2022-03-07 12:01:33.532381: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n","  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n","  function_optimizer: function_optimizer did nothing. time = 0ms.\n","\n","2022-03-07 12:01:33.599034: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\n","2022-03-07 12:01:33.599066: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\n","2022-03-07 12:01:33.631546: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"]}],"source":["convert_to_TF_Lite_float32(model_file_name)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3532,"status":"ok","timestamp":1646079905245,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"ywPEVU5Lk-d8","outputId":"59635c4a-2a07-475c-935c-18a5fb4cf1ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: /tmp/tmpoe8s7o6d/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /tmp/tmpoe8s7o6d/assets\n","2022-03-07 12:01:35.687783: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n","2022-03-07 12:01:35.687945: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n","2022-03-07 12:01:35.689489: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n","  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n","  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n","\n","2022-03-07 12:01:35.749418: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\n","2022-03-07 12:01:35.749482: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\n"]},{"name":"stdout","output_type":"stream","text":["model_8x16x6.h5 has been converted to TF Lite (int8).\n"]}],"source":["convert_to_TF_Lite_int8(model_file_name = model_file_name, val_X = val_X)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125800,"status":"ok","timestamp":1646080034866,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"deb1XDeKk-d8","outputId":"126be1d1-e7ba-4c45-c92f-6049a6db7802"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sensors = 6\n","MSE = 0.013570446892428928\n","Ave error = 0.09779022333243796\n","Max error = 0.49539211998079113\n","Accuracy = 0.7940972222222222\n"]}],"source":["# Load TFLite model and allocate tensors.\n","interpreter = tf.lite.Interpreter(model_path = model_file_name + \"_i8\" + '.tflite')\n","interpreter.allocate_tensors()\n","\n","mse_lite, ave_error_lite, max_error_lite, accuracy_lite = evaluate_regression_TF_Lite(interpreter = interpreter, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","\n","\n","print(\"Sensors = {}\".format(channel_sensors))\n","print(\"MSE = {}\".format(mse_lite))\n","print(\"Ave error = {}\".format(ave_error_lite))\n","print(\"Max error = {}\".format(max_error_lite))\n","print(\"Accuracy = {}\".format(accuracy_lite))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":597646,"status":"error","timestamp":1646083229991,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"rooPCx--k-d8","outputId":"72e68dd9-aa72-44e0-9e79-7545d9ca8b74"},"outputs":[],"source":["mantissa_bits = 1\n","exponent_bits = 4\n","\n","refine_on_batch = True # Set True when you have a low convergence rate (refinement)\n","quantize_enable = True\n","\n","quantization_max_degradation = -0.1 # Percentage\n","#####################################################################################\n","\n","print (\"_______ Post-training quantization _______\")\n","model = load_model(model_file_name)\n","pre_mse_, ave_error_, max_error_, pre_accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","\n","print(\"Original validation MSE = {}.\".format(pre_mse_))\n","\n","quantize_model(model, exponent_bits, mantissa_bits)\n","# evaluate model\n","current_mse_, ave_error_, max_error_, current_accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","print_model (model)\n","print(\"Post-training quantization MSE = {}\".format(current_mse_))\n","\n","print(\"Approximate floating-point quantization. Exponent = {}, Mantissa = {}.\".format(exponent_bits, mantissa_bits))\n","\n","print (\"_______ Quantize aware training _______\")\n","\n","quantize_training_epochs = 100\n","quantize_training_patience = 10\n","training_batch_size = 1000\n","\n","quantize_loop = True\n","while quantize_loop:\n","    print (\"Starting...\")\n","    model = load_model(model_file_name)\n","    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=quantize_training_patience, verbose=1, mode='auto', restore_best_weights=True) \n","    history = model.fit(train_X, train_Y, epochs=quantize_training_epochs, batch_size=training_batch_size, validation_data=(test_X, test_Y), verbose=1, callbacks=[QuantizeCallback(model_file_name = model_file_name, exponent_bits = exponent_bits, mantissa_bits = mantissa_bits, current_mse = current_mse_,threshold_accuracy = current_accuracy, refine_on_batch = refine_on_batch, quantize_enable = quantize_enable), monitor])\n","    # learning curves\n","    optimization_plot(history, \"quantized_optimization\")\n","    model = load_model(model_file_name)\n","    # evaluate model\n","    current_mse_, ave_error_, max_error_, current_accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","    print(\"Quantize aware training MSE = {}\".format(current_mse_))\n","    quantize_loop = current_mse_ < pre_mse_ - quantization_max_degradation * pre_mse_\n","    \n","print('Ending quantized aware training > %.3f' % (current_accuracy * 100.0))"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5714,"status":"ok","timestamp":1646083238961,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"2zU12i_Uk-d8","outputId":"f9d4284d-46bd-4eaa-b0ff-bb1cdf659bca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sensors = 6\n","MSE = 0.013908633450845628\n","Ave error = 0.09882362277011056\n","Max error = 0.48310075258570545\n","Accuracy = 0.7871944444444444\n"]}],"source":["model = load_model(model_file_name)\n","\n","mse, ave_error, max_error, accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","\n","print(\"Sensors = {}\".format(channel_sensors))\n","print(\"MSE = {}\".format(mse))\n","print(\"Ave error = {}\".format(ave_error))\n","print(\"Max error = {}\".format(max_error))\n","print(\"Accuracy = {}\".format(accuracy))"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":376,"status":"aborted","timestamp":1646077432856,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"iiizaGVtk-d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f10aab015e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","Input sample = validation_set/P_x7_y8/sample_2251.npy, model prediction = [[0.51568025 0.68654996]]\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["category_name = \"P_x7_y8\"\n","\n","file = validation_set_path + \"/\" + category_name + \"/sample_2251.npy\"\n","\n","sample = np.load(file)\n","sample = sample.astype('float32') / 255.0\n","sample = np.expand_dims(sample, axis=0)\n","\n","model = load_model(model_file_name)\n","\n","model_output = model.predict(sample)\n","\n","if model_type == \"regression\":\n","    print(\"Input sample = {}, model prediction = {}\".format(file, model_output))\n","else:\n","    print(\"Input sample = {}, model prediction = {}\".format(file, get_category(model_output)))\n","\n","tensor = np.zeros((1, 2))\n","\n","x_i = int(category_name.split(\"_\")[1].split(\"x\")[1]) - 1\n","y_i = int(category_name.split(\"_\")[2].split(\"y\")[1]) - 1\n","\n","x_pos = 0.0935 * x_i + 0.012\n","y_pos = 0.0973 * y_i + 0.012\n","tensor[0,:] = [x_pos, y_pos]\n","\n","plt.scatter(train_Y[:,0], train_Y[:,1], c='gray')\n","plt.scatter(tensor[:,0], tensor[:,1], c='skyblue')\n","plt.scatter(model_output[:,0], model_output[:,1], c='blue')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":377,"status":"aborted","timestamp":1646077432857,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"_JRQmiMjk-d9"},"outputs":[],"source":["show_grid(sample = 280, channels = 3, crop_area = crop_area, size = size)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":376,"status":"aborted","timestamp":1646077432857,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"AwIV7naKk-d9"},"outputs":[],"source":["show_sample(class_name = \"P_x10_y9\", sample = 280, crop_area = crop_area, size = size)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"model.ipynb","provenance":[]},"interpreter":{"hash":"3ba63467901cd6d3991f497c38810e6d1156dd2dfb6eb0edc80f01dd9606bacd"},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
