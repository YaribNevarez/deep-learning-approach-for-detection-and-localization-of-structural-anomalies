{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3163,"status":"ok","timestamp":1646076998365,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"8eYMSpm0k-dr"},"outputs":[],"source":["import os\n","import zipfile\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import math\n","import tensorflow as tf\n","\n","from PIL import Image\n","from matplotlib import pyplot\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, SeparableConv2D, BatchNormalization, MaxPooling2D, Dense, Dropout, Flatten, Activation\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.models import load_model"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1048,"status":"ok","timestamp":1646079701776,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"5qpkVaFBk-dv"},"outputs":[],"source":["def unzip_samples(file = \"data.zip\", overwrite = False):\n","    if not os.path.exists(os.path.splitext(file)[0]) or overwrite:\n","        with zipfile.ZipFile(file, 'r') as zip_ref:\n","            zip_ref.extractall(\".\")\n","\n","def generate_datasets(zip_data_path = \"data.zip\", train_set_path = \"train_set\", train_set_size = 200, test_set_path = \"test_set\", test_set_size = 50, validation_set_path = \"validation_set\", validation_set_size = 50, channels = 4, crop_area = (0, 0, 32, 32), size = (32, 32), unzip = True):\n","    if unzip:\n","        unzip_samples(zip_data_path)\n","    generate_tensors(train_set_path, sample_start = 1, sample_end = train_set_size, channels = channels, crop_area = crop_area, size = size)\n","    generate_tensors(test_set_path, sample_start = train_set_size + 1, sample_end = train_set_size + test_set_size, channels = channels, crop_area = crop_area, size = size)\n","    generate_tensors(validation_set_path, sample_start = train_set_size + test_set_size + 1, sample_end = train_set_size + test_set_size + validation_set_size, channels = channels, crop_area = crop_area, size = size)\n","\n","def sample_path(class_name, sample, channel, size):\n","    png_path = \"dataset\" + str(size[1]) + \"x\" + str(size[0]) + \"/\" + class_name + \"/CH\" + str(channel) + \"/sample\" + str(sample) + \".png\"\n","    return png_path\n","\n","def show_sample(class_name, sample, crop_area = (0, 0, 32, 32), size = (32, 32)):\n","    print(\"Sample = {}, class = {}\".format(sample, class_name))\n","    fig, ax = plt.subplots(1, 7)\n","    fig.set_figwidth(20)\n","    fig.set_figheight(10)\n","    for i in range(7):\n","        im = Image.open(sample_path(class_name = class_name, sample = sample, channel = i+1), \"r\").crop(crop_area).resize(size)\n","        ax[i].set_title(\"Ch {}\".format(i+1))\n","        ax[i].imshow(im)\n","    fig.show()\n","\n","def show_grid(sample, channels = 4, crop_area = (0, 0, 32, 32), size = (32, 32)):\n","    print(\"Grid (sample = {}, channels = {})\".format(sample, channels))\n","    fig, ax = plt.subplots(10, 10)\n","    fig.set_figwidth(20)\n","    fig.set_figheight(24)\n","    matrix = np.zeros((size[1], size[0], channels), dtype=\"uint8\")\n","    for row in range(10):\n","        for col in range(10):\n","            if (row == 0 and col == 0) or (row == 0 and col == 9) or (row == 9 and col == 0) or (row == 9 and col == 9):\n","                    continue\n","            class_name = \"P_x\" + str(row + 1) + \"_y\" + str(col + 1)\n","            for ch in range(channels):\n","                matrix[:, :, ch] = np.array(Image.open(sample_path(class_name = class_name, sample = sample, channel = ch + 1), \"r\").crop(crop_area).resize(size))\n","            ax[row, col].set_title(class_name)\n","            ax[row, col].imshow(matrix)\n","    plt.show()\n","\n","def generate_sample_tensor(class_name = \"A11\", sample = 1, channels = 4, crop_area = (0, 0, 32, 32), size = (32, 32)):\n","    tensor = np.zeros((size[1], size[0], channels), dtype=\"uint8\")\n","    for ch in range(channels):\n","        img = Image.open(sample_path(class_name = class_name, sample = sample, channel = ch + 1, size = size), \"r\")\n","        img = img.crop(crop_area)\n","        img = img.resize(size)\n","        tensor[:, :, ch] = np.array(img)\n","    return tensor\n","\n","def generate_tensors(dest_path = \"tensors\", sample_start = 1, sample_end = 300, channels = 4, crop_area = (0, 0, 32, 32), size = (32, 32), rows = 10, cols = 10, overwrite = False):\n","    if not os.path.exists(dest_path) or overwrite:\n","        for row in range(rows):\n","            for col in range(cols):\n","                if (row == 0 and col == 0) or (row == 0 and col == 9) or (row == 9 and col == 0) or (row == 9 and col == 9):\n","                    continue\n","                class_name = \"P_x\" + str(col + 1) + \"_y\" + str(row + 1)\n","                path = os.path.join(dest_path, class_name)\n","                if not os.path.exists(path):\n","                    os.makedirs(path)\n","                for i in range(sample_start, sample_end + 1):\n","                    file_name = os.path.join(path, \"sample_\" + str(i))\n","                    tensor = generate_sample_tensor(class_name = class_name, sample = i, channels = channels, crop_area = crop_area, size = size)\n","                    np.save(file_name, tensor)\n","\n","def create_labeled_data(srs_path = \"tensors\", label_type = \"categorical\", category_group = None):\n","    labeled_data = []\n","    categories = []\n","    \n","    if category_group != None:\n","        categories = category_group\n","    else:\n","        categories = os.listdir(srs_path)\n","        categories.sort()\n","\n","    for category in categories:\n","        path = os.path.join(srs_path, category)  # create path to the classes\n","        class_num = categories.index(category)  # get the classification  0 = P_x10_y2, 1 = A12, 2 = A13, ...etc\n","\n","        file_list = os.listdir(path)\n","        file_list.sort()\n","        for file in file_list:\n","            file = os.path.join(path, file)\n","            sample = np.load(file)\n","            labeled_data.append([sample, class_num])\n","\n","    random.shuffle(labeled_data)\n","\n","    X = []\n","    Y = []\n","    for sample, label in labeled_data:\n","        X.append(sample)\n","        Y.append(label)\n","\n","    X = np.array(X)\n","    X = X.astype(\"float32\")/255\n","\n","    if label_type == \"regression\":\n","        Y = to_regression(Y, categories)\n","    else:\n","        Y = np.array(Y)\n","        np.reshape(Y, (len(Y), 1))\n","        Y = to_categorical(Y)\n","\n","    return X, Y\n","\n","def to_regression(categorical_labels, categories):\n","    tensor = np.zeros((len(categorical_labels), 2))\n","    for i in range(len(categorical_labels)):\n","        category_name = categories[categorical_labels[i]]\n","\n","        x_i = int(category_name.split(\"_\")[1].split(\"x\")[1]) - 1\n","        y_i = int(category_name.split(\"_\")[2].split(\"y\")[1]) - 1\n","\n","        x_pos = 0.0935 * x_i + 0.012\n","        y_pos = 0.0973 * y_i + 0.012\n","        tensor[i,:] = [x_pos, y_pos]\n","    return tensor\n","\n","def get_category(distributions, max_row = 4, max_col = 4):\n","    category_index = distributions.argmax()\n","\n","    col = category_index % max_col\n","    row = int((category_index - col) / max_row)\n","    category = \"A\" + str(row + 1) + str(col + 1)\n","    return category\n","\n","def optimization_plot(history, filename):\n","    # plot loss\n","    f = pyplot.figure()\n","    f.set_figwidth(10)\n","    f.set_figheight(20)\n","\n","    pyplot.subplot(211)\n","    pyplot.title('Cross Entropy Loss')\n","    pyplot.plot(history.history['loss'], color='blue', label='train')\n","    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n","    # plot accuracy\n","    pyplot.subplot(212)\n","    pyplot.title('Regression MSE')\n","    pyplot.plot(history.history['mean_squared_error'], color='blue', label='train')\n","    pyplot.plot(history.history['val_mean_squared_error'], color='orange', label='test')\n","    # save plot to file\n","    pyplot.savefig(filename + '.png')\n","    pyplot.close()\n","\n","def compute_error(error_tensor):\n","    mse = ((error_tensor)**2).mean(axis=None)\n","    max_error = np.max(error_tensor)\n","    ave_error = sum(error_tensor) / len(error_tensor)\n","    return mse, ave_error, max_error\n","\n","def evaluate_regression(model, val_X, val_Y, accuracy_threshold):\n","    model_output = model.predict(val_X)\n","    error_tensor = val_Y - model_output\n","    error_x_tensor = error_tensor[:,0]\n","    error_y_tensor = error_tensor[:,1]\n","    loss_distance = np.sqrt(error_x_tensor**2 + error_y_tensor**2)\n","    mse, ave_error, max_error = compute_error (loss_distance)\n","    accuracy = np.count_nonzero(loss_distance < accuracy_threshold) / len(loss_distance)\n","    return mse, ave_error, max_error, accuracy\n","\n","def evaluate_regression_TF_Lite(interpreter, val_X, val_Y, accuracy_threshold):\n","    # Get input and output tensors.\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","\n","    val_Y_lite = np.zeros((len(val_Y), 2))\n","    for i in range(len(val_X)):\n","        interpreter.set_tensor(input_details[0]['index'], np.expand_dims(val_X[i], axis=0))\n","        interpreter.invoke()\n","        output_data = interpreter.get_tensor(output_details[0]['index'])\n","        val_Y_lite[i] = output_data\n","\n","    error_tensor_lite = val_Y - val_Y_lite\n","\n","    error_x_tensor_lite = error_tensor_lite[:,0]\n","    error_y_tensor_lite = error_tensor_lite[:,1]\n","\n","    loss_distance_lite = np.sqrt(error_x_tensor_lite**2 + error_y_tensor_lite**2)\n","\n","    accuracy_lite = np.count_nonzero(loss_distance_lite < accuracy_threshold) / len(loss_distance_lite)\n","    mse_lite, ave_error_lite, max_error_lite = compute_error(loss_distance_lite)\n","    return mse_lite, ave_error_lite, max_error_lite, accuracy_lite\n","\n","def train_model(model, model_file_name = \"model.h5\", target_mse = 0.000001, accuracy_threshold = 0.10):\n","    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto', restore_best_weights=True)\n","\n","    current_mse = 1\n","    while (target_mse < current_mse):\n","        history = model.fit(train_X, train_Y, epochs=100, batch_size=1000, validation_data=(val_X, val_Y), verbose=1, callbacks=[monitor])\n","        validation_mse_, ave_error_, max_error_, validation_accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","        if (validation_mse_ < current_mse):\n","            print(\"Validation MSE > {}\".format(validation_mse_))\n","            print(\"Saving model > {}\".format(model_file_name))\n","            model.save(model_file_name)\n","            optimization_plot(history, \"optimization\")\n","            current_mse = validation_mse_\n","        else:\n","            if os.path.exists(model_file_name):\n","                model = load_model(model_file_name)\n","\n","    print(\"{} has been trained, MSE = {}.\".format(model_file_name, current_mse))\n","\n","def convert_to_TF_Lite_float32 (model_file_name):\n","  model = load_model(model_file_name)\n","\n","  # Convert the model.\n","  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","  converter.inference_input_type = tf.float32  # or tf.uint8\n","  converter.inference_output_type = tf.float32  # or tf.uint8\n","  tflite_model = converter.convert()\n","\n","  # Save the model.\n","  with open(model_file_name + '.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","\n","  os.system(\"xxd -i \" + model_file_name + '.tflite' + \" > \" + model_file_name + '.cpp')\n","\n","  print(\"{} has been converted to TF Lite (float32).\".format(model_file_name))\n","\n","def convert_to_TF_Lite_int8 (model_file_name, val_X):\n","  ds_cast = tf.cast(val_X, tf.float32)\n","  ds_batch = tf.data.Dataset.from_tensor_slices((ds_cast)).batch(1)\n","  def representative_dataset():\n","    for input_value in ds_batch.take(100):\n","      # Model has only one input so each data point has one element.\n","      yield [input_value]\n","\n","  model = load_model(model_file_name)\n","  # Convert the model.\n","  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","  converter.representative_dataset = representative_dataset\n","  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","  converter.inference_input_type = tf.float32  # or tf.uint8\n","  converter.inference_output_type = tf.float32  # or tf.uint8\n","  tflite_model = converter.convert()\n","\n","  # Save the model.\n","  with open(model_file_name + \"_i8\" + '.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","\n","  print(\"{} has been converted to TF Lite (int8).\".format(model_file_name))\n","\n","def create_custom_training_set (srs_path, tokens):\n","    categories = os.listdir(srs_path)\n","    training_categories = []\n","    validation_categories = []\n","    for category in categories:\n","        is_train_category = False\n","        for token in tokens:\n","            if token in category:\n","                training_categories.append(category)\n","                is_train_category = True\n","        if not is_train_category:\n","            validation_categories.append(category)\n","\n","    return list (dict.fromkeys(training_categories)), list (dict.fromkeys(validation_categories))\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":358,"status":"ok","timestamp":1646080824052,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"Mxi14fO0k-d0"},"outputs":[],"source":["import struct\n","\n","def bin2float(b):\n","    ''' Convert binary string to a float.\n","\n","    Attributes:\n","        :b: Binary string to transform.\n","    '''\n","    h = int(b, 2).to_bytes(8, byteorder=\"big\")\n","    return struct.unpack('>d', h)[0]\n","\n","\n","def float2bin(f):\n","    ''' Convert float to 64-bit binary string.\n","\n","    Attributes:\n","        :f: Float number to transform.\n","    '''\n","    [d] = struct.unpack(\">Q\", struct.pack(\">d\", f))\n","    return f'{d:064b}'\n","\n","def quantize_float (float_number, exponent_bits, mantissa_bits):\n","    exponent_sign = 1\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n","    if mantissa_bits < 0:\n","      mantissa_bits = 0\n","    if exponent_bits < 0:\n","      exponent_bits = 0\n","      \n","    coefficient = float2bin(float_number)\n","    sign = int (coefficient[:1], 2)\n","    if 0 < mantissa_bits:\n","        custom_mantissa = int (coefficient[12 : 12 + mantissa_bits], 2)\n","    else:\n","        custom_mantissa = 0\n","    residual_mantissa = int (coefficient[12 + mantissa_bits:], 2)\n","    exponent = int (coefficient[1:12], 2) - 1023\n","\n","    exponent_full_range = pow(2, exponent_bits - exponent_sign) - 1\n","    if exponent < - exponent_full_range:\n","        quantized_value = 0\n","    elif exponent > exponent_full_range:\n","        quantized_value = pow(-1, sign) * (1 + (1 - pow(2, - mantissa_bits))) * pow(2, exponent_full_range)\n","    else:\n","        if (pow (2, (52 - (mantissa_bits + 1))) - 1) < residual_mantissa:\n","            custom_mantissa += 1\n","            if (pow (2, mantissa_bits) - 1) < custom_mantissa:\n","                custom_mantissa = 0\n","                exponent += 1\n","    \n","        quantized_value = pow(-1, sign) * (1 + custom_mantissa * pow(2, - mantissa_bits)) * pow(2, exponent)\n","    return quantized_value\n","\n","def quantize_model(model, exponent_bits, mantissa_bits):\n","  for layer in model.layers:\n","    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.SeparableConv2D):\n","      layer_weights = layer.get_weights()\n","      for index in range(len(layer_weights)):\n","        matrix = layer_weights[index]\n","        for weight_index, weight in np.ndenumerate(matrix):\n","          matrix[weight_index] = quantize_float(weight, exponent_bits, mantissa_bits)\n","        layer_weights[index] = matrix\n","      layer.set_weights(layer_weights)\n","\n","class QuantizeCallback(tf.keras.callbacks.Callback):\n","  def __init__(self, model_file_name, exponent_bits, mantissa_bits, current_mse, threshold_accuracy, refine_on_batch, quantize_enable):\n","    self.model_file_name = model_file_name\n","    self.threshold_accuracy = threshold_accuracy\n","    self.current_mse = current_mse\n","    self.refine_on_batch = refine_on_batch\n","    self.exponent_bits = exponent_bits\n","    self.mantissa_bits = mantissa_bits\n","    self.quantize_enable = quantize_enable\n","\n","  def on_epoch_end(self, epoch, logs = None):\n","    if self.quantize_enable:\n","      quantize_model(self.model, self.exponent_bits, self.mantissa_bits)\n","    test_mse_, ave_error_, max_error_, test_accuracy = evaluate_regression(model = self.model, val_X = val_X, val_Y = val_Y, accuracy_threshold = self.threshold_accuracy)\n","    if (test_mse_ < self.current_mse):\n","      self.model.save(self.model_file_name)\n","      self.current_mse = test_mse_\n","      print(\" Saveing model ...\")\n","\n","    print(\" Regression MSE = {}\".format(test_mse_))\n","  def on_train_end(self, logs = None):\n","    if self.quantize_enable:\n","      quantize_model(self.model, self.exponent_bits, self.mantissa_bits)\n","  def on_batch_end(self, epoch, logs = None):\n","    if self.quantize_enable:\n","      quantize_model(self.model, self.exponent_bits, self.mantissa_bits)\n","\n","    if self.refine_on_batch:\n","      test_mse_, ave_error_, max_error_, test_accuracy = evaluate_regression(model = self.model, val_X = val_X, val_Y = val_Y, accuracy_threshold = self.threshold_accuracy)\n","      if (test_mse_ < self.current_mse):\n","        self.model.save(self.model_file_name)\n","        self.current_mse = test_mse_\n","        print(\" Saveing model ...\")\n","        print(\" Regression MSE {}\".format(test_mse_))\n","\n","def print_model(model):\n","  for layer in model.layers:\n","    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.SeparableConv2D):\n","      layer_weights = layer.get_weights()\n","      print (layer_weights)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1646076999098,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"1U3sJkhSk-d2"},"outputs":[],"source":["def CNN_regression(input_shape = (32, 32, 4), output_regression = 2):\n","    model = Sequential()\n","\n","    model.add(Conv2D(40, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape = input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Dropout(0.3))\n","\n","    model.add(Conv2D(60, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Dropout(0.3))\n","\n","    model.add(Conv2D(120, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D((2, 2)))\n","    model.add(Dropout(0.3))\n","\n","    model.add(Flatten())\n","    model.add(Dense(32, activation='relu'))\n","    model.add(Dense(output_regression, activation='sigmoid'))\n","    # compile model\n","    #opt = SGD(learning_rate=0.001, momentum=0.9)\n","    opt = \"adam\"\n","    model.compile(optimizer=opt, loss='mean_squared_error', metrics=['MeanSquaredError'])\n","    return model"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1823,"status":"ok","timestamp":1646077000913,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"B2YTXhvbk-d3","outputId":"732139c7-bd58-4dfc-956f-53346dc76f6e"},"outputs":[],"source":["google_service = False\n","\n","absolute_path = \"\"\n","\n","if google_service:\n","    from google.colab import drive\n","    google_drive_path = \"/content/drive/MyDrive/PhD/Minized/\"\n","    drive.mount('/content/drive/')\n","    absolute_path = google_drive_path\n","\n","channel_sensors = 6\n","\n","temporal_resolution = 16\n","\n","model_file_name = absolute_path + \"model_8x\" + str(temporal_resolution) + \"x\" + str(channel_sensors) + \".h5\" # Set the file name for the model\n","target_mse = 0.010\n","accuracy_threshold = 0.15 # Radio-distance (in meters) to consider a correct regression\n","\n","model_type = \"regression\" # set \"regression\" or \"categorical\"\n","\n","zip_data_path = absolute_path + \"dataset8x\" + str(temporal_resolution) + \".zip\"\n","unzip_dataset = True\n","\n","train_set_path = absolute_path + \"train_set\"\n","train_set_size = 1700\n","\n","test_set_path = absolute_path + \"test_set\"\n","test_set_size = 400\n","\n","validation_set_path = absolute_path + \"validation_set\"\n","validation_set_size = 400\n","\n","crop_area = (0, 0, temporal_resolution, 8) # x1, y1, x2, y2\n","size = (temporal_resolution, 8)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":261681,"status":"ok","timestamp":1646077262588,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"bf6wEFzIk-d3"},"outputs":[],"source":["generate_datasets ( zip_data_path = zip_data_path,\\\n","                    train_set_path = train_set_path,\\\n","                    train_set_size = train_set_size,\\\n","                    test_set_path = test_set_path,\\\n","                    test_set_size = test_set_size,\\\n","                    validation_set_path = validation_set_path,\\\n","                    validation_set_size = validation_set_size,\\\n","                    channels = channel_sensors,\\\n","                    crop_area = crop_area,\\\n","                    size = size,\n","                    unzip = unzip_dataset)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1646077262590,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"xnShZa-Vk-d4"},"outputs":[],"source":["#tokens = [\"x1_\", \"x10_\", \"_y1\", \"_y10\", \"x5_\", \"_y5\"]\n","tokens = [\"x5_y5\", \"x1_y5\", \"x3_y3\", \"x3_y7\", \"x10_y5\", \"x7_y3\", \"x7_y7\", \"x5_y1\", \"x5_y10\", \"x2_y2\", \"x2_y9\", \"x9_y2\", \"x9_y9\"]\n","\n","training_group, validation_group = create_custom_training_set (srs_path = train_set_path, tokens = tokens)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":138259,"status":"ok","timestamp":1646077400836,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"PWMMyapzk-d4"},"outputs":[],"source":["train_X, train_Y = create_labeled_data(srs_path = train_set_path, label_type = model_type, category_group = training_group)\n","test_X, test_Y = create_labeled_data(srs_path = test_set_path, label_type = model_type, category_group = validation_group)\n","val_X, val_Y = create_labeled_data(srs_path = validation_set_path, label_type = model_type, category_group = validation_group)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1646077400837,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"lqoYYYWpk-d5","outputId":"b0f66234-2de9-499d-ae3e-f5b1572d180f"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.scatter(train_Y[:,0], train_Y[:,1], c='r')\n","plt.show()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2330,"status":"ok","timestamp":1646077403162,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"6VUMfV_Qk-d6","outputId":"1fe22b8d-5f68-4740-fdd7-69e4e6413be4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 8, 16, 40)         2560      \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 8, 16, 40)         160       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 4, 8, 40)          0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 4, 8, 40)          0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 4, 8, 60)          21660     \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 4, 8, 60)          240       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 2, 4, 60)          0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 2, 4, 60)          0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 2, 4, 120)         64920     \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 2, 4, 120)         480       \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 1, 2, 120)         0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 1, 2, 120)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 240)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 32)                7712      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 2)                 66        \n","=================================================================\n","Total params: 97,798\n","Trainable params: 97,358\n","Non-trainable params: 440\n","_________________________________________________________________\n","('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"]},{"name":"stderr","output_type":"stream","text":["2022-03-05 19:01:06.716771: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2022-03-05 19:01:06.719027: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-03-05 19:01:06.721088: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"]}],"source":["if model_type == \"regression\":\n","    model = CNN_regression(input_shape = (size[1], size[0], channel_sensors))\n","else:\n","    model = CNN_categorical(input_shape = (size[1], size[0], channel_sensors), output_categories = 16)\n","\n","model.summary()\n","\n","tf.keras.utils.plot_model(model, model_file_name + \".png\", show_shapes=True, show_layer_names=True, expand_nested=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":169953,"status":"error","timestamp":1646079880611,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"tiq18IU3k-d6","outputId":"091227b7-4eb0-4f2e-af7a-6df6ed0ebea7"},"outputs":[],"source":["train_model(model = model, model_file_name = model_file_name, target_mse = target_mse, accuracy_threshold = accuracy_threshold)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1646079885196,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"IPzSoupPveEa","outputId":"b369e7b7-1875-4923-d310-6adbc14b5bef"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.scatter(train_Y[:,0], train_Y[:,1], c='r')\n","plt.scatter(val_Y[:,0], val_Y[:,1], c='b')\n","plt.show()"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2219,"status":"ok","timestamp":1646079891491,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"6eeHuHxDk-d7","outputId":"f7f0806c-e8a8-4315-a62a-61d25dc05e0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sensors = 7\n","MSE = 0.014162802934192682\n","Ave error = 0.10558654261668629\n","Max error = 0.7238420015538605\n","Accuracy = 0.8433734939759037\n"]}],"source":["model = load_model(model_file_name)\n","\n","mse, ave_error, max_error, accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","\n","print(\"Sensors = {}\".format(channel_sensors))\n","print(\"MSE = {}\".format(mse))\n","print(\"Ave error = {}\".format(ave_error))\n","print(\"Max error = {}\".format(max_error))\n","print(\"Accuracy = {}\".format(accuracy))"]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4828,"status":"ok","timestamp":1646079901718,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"8l69_K0Ek-d7","outputId":"dce4092f-a63b-4f60-b937-24f180123969"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: /tmp/tmpnlw0pv8p/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /tmp/tmpnlw0pv8p/assets\n","2022-03-05 12:05:57.023061: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n","2022-03-05 12:05:57.023912: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n","2022-03-05 12:05:57.041712: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n","  function_optimizer: function_optimizer did nothing. time = 0.005ms.\n","  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n","\n"]},{"name":"stdout","output_type":"stream","text":["model_8x16x6.h5 has been converted to TF Lite (float32).\n"]},{"name":"stderr","output_type":"stream","text":["2022-03-05 12:05:57.194030: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\n","2022-03-05 12:05:57.194284: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\n"]}],"source":["convert_to_TF_Lite_float32(model_file_name)"]},{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3532,"status":"ok","timestamp":1646079905245,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"ywPEVU5Lk-d8","outputId":"59635c4a-2a07-475c-935c-18a5fb4cf1ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: /tmp/tmpm2il84nb/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /tmp/tmpm2il84nb/assets\n","2022-03-05 12:06:01.061105: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n","2022-03-05 12:06:01.061233: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n","2022-03-05 12:06:01.063755: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n","  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n","  function_optimizer: function_optimizer did nothing. time = 0ms.\n","\n","2022-03-05 12:06:01.189552: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\n","2022-03-05 12:06:01.189588: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\n"]},{"name":"stdout","output_type":"stream","text":["model_8x16x6.h5 has been converted to TF Lite (int8).\n"]}],"source":["convert_to_TF_Lite_int8(model_file_name = model_file_name, val_X = val_X)"]},{"cell_type":"code","execution_count":81,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125800,"status":"ok","timestamp":1646080034866,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"deb1XDeKk-d8","outputId":"126be1d1-e7ba-4c45-c92f-6049a6db7802"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sensors = 6\n","MSE = 0.014165605684387942\n","Ave error = 0.10197974020371141\n","Max error = 0.8237342330860195\n","Accuracy = 0.8559939759036145\n"]}],"source":["# Load TFLite model and allocate tensors.\n","interpreter = tf.lite.Interpreter(model_path = model_file_name + \"_i8\" + '.tflite')\n","interpreter.allocate_tensors()\n","\n","mse_lite, ave_error_lite, max_error_lite, accuracy_lite = evaluate_regression_TF_Lite(interpreter = interpreter, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","\n","\n","print(\"Sensors = {}\".format(channel_sensors))\n","print(\"MSE = {}\".format(mse_lite))\n","print(\"Ave error = {}\".format(ave_error_lite))\n","print(\"Max error = {}\".format(max_error_lite))\n","print(\"Accuracy = {}\".format(accuracy_lite))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":597646,"status":"error","timestamp":1646083229991,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"rooPCx--k-d8","outputId":"72e68dd9-aa72-44e0-9e79-7545d9ca8b74"},"outputs":[],"source":["mantissa_bits = 1\n","exponent_bits = 4\n","\n","refine_on_batch = True # Set True when you have a low convergence rate (refinement)\n","quantize_enable = True\n","\n","quantization_max_degradation = -0.1 # Percentage\n","#####################################################################################\n","\n","print (\"_______ Post-training quantization _______\")\n","model = load_model(model_file_name)\n","pre_mse_, ave_error_, max_error_, pre_accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","\n","print(\"Original validation MSE = {}.\".format(pre_mse_))\n","\n","quantize_model(model, exponent_bits, mantissa_bits)\n","# evaluate model\n","current_mse_, ave_error_, max_error_, current_accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","print_model (model)\n","print(\"Post-training quantization MSE = {}\".format(current_mse_))\n","\n","print(\"Approximate floating-point quantization. Exponent = {}, Mantissa = {}.\".format(exponent_bits, mantissa_bits))\n","\n","print (\"_______ Quantize aware training _______\")\n","\n","quantize_training_epochs = 100\n","quantize_training_patience = 10\n","training_batch_size = 100\n","\n","quantize_loop = True\n","while quantize_loop:\n","    print (\"Starting...\")\n","    model = load_model(model_file_name)\n","    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=quantize_training_patience, verbose=1, mode='auto', restore_best_weights=True) \n","    history = model.fit(train_X, train_Y, epochs=quantize_training_epochs, batch_size=training_batch_size, validation_data=(test_X, test_Y), verbose=1, callbacks=[QuantizeCallback(model_file_name = model_file_name, exponent_bits = exponent_bits, mantissa_bits = mantissa_bits, current_mse = current_mse_,threshold_accuracy = current_accuracy, refine_on_batch = refine_on_batch, quantize_enable = quantize_enable), monitor])\n","    # learning curves\n","    optimization_plot(history, \"quantized_optimization\")\n","    model = load_model(model_file_name)\n","    # evaluate model\n","    current_mse_, ave_error_, max_error_, current_accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","    print(\"Quantize aware training MSE = {}\".format(current_mse_))\n","    quantize_loop = current_mse_ < pre_mse_ - quantization_max_degradation * pre_mse_\n","    \n","print('Ending quantized aware training > %.3f' % (current_accuracy * 100.0))"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5714,"status":"ok","timestamp":1646083238961,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"2zU12i_Uk-d8","outputId":"f9d4284d-46bd-4eaa-b0ff-bb1cdf659bca"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sensors = 7\n","MSE = 0.014162802934192682\n","Ave error = 0.10558654261668629\n","Max error = 0.7238420015538605\n","Accuracy = 0.8433734939759037\n"]}],"source":["model = load_model(model_file_name)\n","\n","mse, ave_error, max_error, accuracy = evaluate_regression(model = model, val_X = val_X, val_Y = val_Y, accuracy_threshold = accuracy_threshold)\n","\n","print(\"Sensors = {}\".format(channel_sensors))\n","print(\"MSE = {}\".format(mse))\n","print(\"Ave error = {}\".format(ave_error))\n","print(\"Max error = {}\".format(max_error))\n","print(\"Accuracy = {}\".format(accuracy))"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":376,"status":"aborted","timestamp":1646077432856,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"iiizaGVtk-d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:8 out of the last 1045 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2acba3a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","Input sample = validation_set/P_x9_y3/sample_2251.npy, model prediction = [[0.7118547  0.19385964]]\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["category_name = \"P_x9_y3\"\n","\n","file = validation_set_path + \"/\" + category_name + \"/sample_2251.npy\"\n","\n","sample = np.load(file)\n","sample = sample.astype('float32') / 255.0\n","sample = np.expand_dims(sample, axis=0)\n","\n","model = load_model(model_file_name)\n","\n","model_output = model.predict(sample)\n","\n","if model_type == \"regression\":\n","    print(\"Input sample = {}, model prediction = {}\".format(file, model_output))\n","else:\n","    print(\"Input sample = {}, model prediction = {}\".format(file, get_category(model_output)))\n","\n","tensor = np.zeros((1, 2))\n","\n","x_i = int(category_name.split(\"_\")[1].split(\"x\")[1]) - 1\n","y_i = int(category_name.split(\"_\")[2].split(\"y\")[1]) - 1\n","\n","x_pos = 0.0935 * x_i + 0.012\n","y_pos = 0.0973 * y_i + 0.012\n","tensor[0,:] = [x_pos, y_pos]\n","\n","plt.scatter(train_Y[:,0], train_Y[:,1], c='gray')\n","plt.scatter(tensor[:,0], tensor[:,1], c='skyblue')\n","plt.scatter(model_output[:,0], model_output[:,1], c='blue')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":377,"status":"aborted","timestamp":1646077432857,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"_JRQmiMjk-d9"},"outputs":[],"source":["show_grid(sample = 280, channels = 3, crop_area = crop_area, size = size)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":376,"status":"aborted","timestamp":1646077432857,"user":{"displayName":"Yarib Nevarez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaAxNyhXYIRE_aUYNIDs1gS94f2PAsxaUT0Df9lww=s64","userId":"06137369174830896592"},"user_tz":-60},"id":"AwIV7naKk-d9"},"outputs":[],"source":["show_sample(class_name = \"P_x10_y9\", sample = 280, crop_area = crop_area, size = size)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"model.ipynb","provenance":[]},"interpreter":{"hash":"3ba63467901cd6d3991f497c38810e6d1156dd2dfb6eb0edc80f01dd9606bacd"},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
